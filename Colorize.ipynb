{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colorize.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "WCsZ2nqKxnPn",
        "H-5ZVNVj_8zA",
        "C314mXYeQnG4",
        "P1hnWz4pAX70",
        "1KEUjE_NdI_n",
        "mTc8zqB6KWRk",
        "BskbhNsYKaBO",
        "jP5ki_kDxbWP",
        "nS0KJaHeohd7",
        "piyyaSU6giHn",
        "lV9woQpX09FB",
        "5bskRAaXCSrI",
        "GXRPIeGoEXSL",
        "zP9NLoaULRIa"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPe9B2woea09H6JpLT0qNTJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexsg4/colorize/blob/main/Colorize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wVD6Hv3IdAb"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IUdckYxHevR"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy import io\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import time\n",
        "import timeit \n",
        "import pickle\n",
        "import multiprocessing as mp\n",
        "import gc\n",
        "from collections import OrderedDict\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms as tvt\n",
        "from torchvision import utils as tvu\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofqhf7ntI9dw"
      },
      "source": [
        "### Helper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1SMXJsNI4zr"
      },
      "source": [
        "import IPython\n",
        "# Helper\n",
        "def ring_bell(message=''):\n",
        "  \"\"\" \n",
        "  A helper function that plays a sound and outputs a message. \n",
        "  Used for notifying the completion of various steps i.e. model training.\n",
        "  Should only be run from an IPython environment.\n",
        "  \"\"\"\n",
        "  print(message)\n",
        "  js_code = '''\n",
        "  const audio = new Audio(\"https://www.myinstants.com/media/sounds/epic.mp3\");\n",
        "  audio.pause();\n",
        "  audio.addEventListener(\"canplaythrough\", function () {\n",
        "        setTimeout(function(){\n",
        "            audio.pause();\n",
        "        },\n",
        "        4300);\n",
        "  }, false); \n",
        "  audio.play();\n",
        "  '''\n",
        "  display(IPython.display.Javascript(js_code))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7QupnTqb-lI"
      },
      "source": [
        "### Env"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfMbRQGTcGQa"
      },
      "source": [
        "HOME = '/usr/local/bin/ml-docker-data'\n",
        "DATA_PATH_SUN = os.path.join(HOME, 'SUN397')\n",
        "DATA_PATH_FLICKR = os.path.join(HOME, 'flickr10k-aug')\n",
        "\n",
        "CPUS = mp.cpu_count()\n",
        "WORKERS = CPUS-1\n",
        "CUDA = torch.cuda.is_available()\n",
        "DEVICE = torch.device('cuda') if CUDA else 'cpu'\n",
        "\n",
        "VERBOSE = False #@param{type:'boolean'}\n",
        "\n",
        "print(f'HOME: {HOME}')\n",
        "print(f'\\nSUN397: {DATA_PATH_SUN}')\n",
        "print(f'flickr: {DATA_PATH_FLICKR}')\n",
        "print(f'\\nCPUS: {CPUS} | WORKERS: {WORKERS}')\n",
        "print(f'DEVICE: {DEVICE}')\n",
        "\n",
        "ring_bell('\\ntest beep')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1On2maoO6NO"
      },
      "source": [
        "# Data gathering\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhOWXB3Ycwzb"
      },
      "source": [
        "## 1) Gather color info from SUN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "la83Yw7tgQqK"
      },
      "source": [
        "### Get the SUN urls file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsJnY-eBdAqG"
      },
      "source": [
        "urls_path = os.path.join(DATA_PATH_SUN, 'SUN397_urls.mat')\n",
        "if not os.path.isfile(urls_path):\n",
        "  ! curl -s https://vision.cs.princeton.edu/projects/2010/SUN/urls/SUN397_urls.mat > {urls_path}\n",
        "SUN = io.loadmat(urls_path)['SUN']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCzkxm8agVC6"
      },
      "source": [
        "### Choose the categories to keep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pe4G-l5ogZy6"
      },
      "source": [
        "DATA_PATH_SUN = os.path.join(HOME, 'SUN397')\n",
        "\n",
        "kept_cats = []\n",
        "with open(os.path.join(DATA_PATH_SUN, 'kept_cats-small.txt'), 'r') as fp:\n",
        "  for line in fp:\n",
        "    if line.find('#') == -1:\n",
        "      line = line.replace('\\n', '')\n",
        "      kept_cats.append(line)\n",
        "\n",
        "print(f'categories to keep: {len(kept_cats)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9ZwA6CGEdHu"
      },
      "source": [
        "### Query helper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fo8Is5T6got4"
      },
      "source": [
        "def query_sun_filepaths(data_path, cats_to_keep, max_img_per_cat):\n",
        "  # build a list of file_paths\n",
        "  fpaths = []\n",
        "\n",
        "  # number of samples to keep from every category\n",
        "  imgs_per_cat = {cat:0 for cat in cats_to_keep}\n",
        "\n",
        "  with open(os.path.join(data_path, 'fpath.txt'), 'r') as pathfile:\n",
        "    for line in pathfile:\n",
        "      img_path = line.replace('.', os.path.join(data_path, 'images'), 1)[:-1]\n",
        "      \n",
        "      if not os.path.isfile(img_path):\n",
        "        continue\n",
        "      \n",
        "      kept_cat = None\n",
        "      for cat in kept_cats:\n",
        "        if img_path.find(cat) != -1 and imgs_per_cat[cat] < max_img_per_cat:\n",
        "          kept_cat = cat\n",
        "          break\n",
        "\n",
        "      if kept_cat is None:\n",
        "        continue\n",
        "      \n",
        "      fpaths.append(img_path)\n",
        "      imgs_per_cat[kept_cat] += 1\n",
        "\n",
        "    # sanity check\n",
        "    if VERBOSE:\n",
        "      for cat, num in imgs_per_cat.items():\n",
        "        print(f'{cat}: {num}')\n",
        "      print('\\n')\n",
        "\n",
        "    print(f'total images: {len(fpaths)}')\n",
        "      \n",
        "  return fpaths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0ynTNY0hhmY"
      },
      "source": [
        "### Gather UV data from the images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDjULRyrhmi5"
      },
      "source": [
        "#### Helpers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMcwp4UVhlzg"
      },
      "source": [
        "def compute_uvs_for_imgs_local(fpaths, max_imgs, resize, id, disp_every=50):\n",
        "  start_t = time.time()\n",
        "\n",
        "  num_imgs = 0\n",
        "\n",
        "  Us = []\n",
        "  Vs = []\n",
        "\n",
        "  # for every file path\n",
        "  for fpath in fpaths:\n",
        "    # load the image from disk\n",
        "    try:\n",
        "      image = cv2.imread(fpath)\n",
        "    except FileNotFoundError as e:\n",
        "      print(f'#{id}: {e} | path: {fpath}')\n",
        "      continue\n",
        "\n",
        "    h, w = image.shape[:2]\n",
        "    short_edge = min(w, h)\n",
        "    if resize > 0 and short_edge > resize:\n",
        "      p = resize * 100 / short_edge \n",
        "      new_size = (int(w * p / 100), int(h * p / 100))\n",
        "      image = cv2.resize(image, new_size, interpolation = cv2.INTER_LANCZOS4)\n",
        "      if VERBOSE:\n",
        "        print(f'#{id}: resized image to:', image.shape)\n",
        "\n",
        "    # store it as a numpy array and check it's 3 channels\n",
        "\n",
        "    if len(image.shape) != 3 or image.shape[-1] != 3:\n",
        "      if VERBOSE:\n",
        "        print(f'#{id}: skipping grayscale image:', fpath)\n",
        "      continue\n",
        "          \n",
        "    if VERBOSE:\n",
        "      print(f'#{id}:\\t read image from:', fpath)\n",
        "      print(f'#{id}:\\t num_imgs', num_imgs)\n",
        "      print(f'#{id}:\\t max_imgs', max_imgs)\n",
        "\n",
        "\n",
        "    # normalize image to 0-1\n",
        "    image = np.float32(image * 1/255.)\n",
        "\n",
        "    # convert BGR image to YUV\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2YCrCb)\n",
        "\n",
        "    U = image[..., 1]\n",
        "    V = image[..., 2]\n",
        "\n",
        "    Us.extend(U.flatten())\n",
        "    Vs.extend(V.flatten())\n",
        "    num_imgs += 1\n",
        "\n",
        "    if num_imgs % disp_every == 0:\n",
        "      print(f'#{id}:\\t computed UVs for {num_imgs} images')\n",
        "\n",
        "    if num_imgs == max_imgs:\n",
        "      print(f'#{id}:\\t computed UVs for {num_imgs}(max) images')\n",
        "      duration = time.time() - start_t\n",
        "\n",
        "      return Us, Vs, id, duration\n",
        "    \n",
        "  print(f'#{id}:\\t computed uvs for {num_imgs}(done) imgs, exiting...')\n",
        "  duration = time.time() - start_t\n",
        "\n",
        "  return Us, Vs, id, duration"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIk8G42mhogj"
      },
      "source": [
        "#### Get the uv data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ql5ULGmQiIrC"
      },
      "source": [
        "VERBOSE = False\n",
        "MAX_IMG_PER_CAT =  1000#@param{type:'integer'}\n",
        "\n",
        "fpath = query_sun_filepaths(DATA_PATH_SUN, kept_cats, MAX_IMG_PER_CAT)\n",
        "\n",
        "# sanity check\n",
        "#for img in fpath[:3]:\n",
        "  #print(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF6_EHgqF_Ba"
      },
      "source": [
        "MAX_IMG =  100#@param{type:'integer'}\n",
        "MAX_SIZE = 512 #@param{type:'integer'}\n",
        "\n",
        "print('computing uv pixel data...')\n",
        "Us, Vs, id, duration = compute_uvs_for_imgs_local(fpath, max_imgs = MAX_IMG, \n",
        "                                                  resize = MAX_SIZE, \n",
        "                                                  id = 0, disp_every = 50)\n",
        "print(f'duration: {duration:.2f} sec')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GnkLgJ502hi"
      },
      "source": [
        "print(np.min(Us), np.max(Us))\n",
        "print(np.min(Vs), np.max(Vs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy0YSFe7i-RG"
      },
      "source": [
        "### Compute weights based on the probabilities for each bin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6UTzIHljBHb"
      },
      "source": [
        "# number of bins to split the space in\n",
        "NUM_BINS = 10\n",
        "\n",
        "# edges of the histogram\n",
        "x_edge = np.linspace(-0.1, 1.1, NUM_BINS + 1)\n",
        "y_edge = np.linspace(-0.1, 1.1, NUM_BINS + 1)\n",
        "\n",
        "hist, _, _ = np.histogram2d(Us, Vs, bins = [x_edge, y_edge])\n",
        "hist /= np.sum(hist)\n",
        "print('computed normalized histogram')\n",
        "print(np.min(hist))\n",
        "print(np.max(hist))\n",
        "\n",
        "del Us\n",
        "del Vs\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAvfJlPNjZPJ"
      },
      "source": [
        "THRESHOLD = 1e-7\n",
        "LAMBDA = 0.4\n",
        "MAX_CATEGORIES = 6\n",
        "\n",
        "cat2id = {}\n",
        "id2cat = {}\n",
        "cat_freq = {}\n",
        "\n",
        "sorted_values = np.sort(np.ravel(hist))[::-1]\n",
        "# determine how many categories are above the threshold\n",
        "NUM_CAT = sorted_values.shape[0] - np.searchsorted(sorted_values[::-1], THRESHOLD)\n",
        "print('num_cat initial', NUM_CAT)\n",
        "\n",
        "# adjust threshold to only keep MAX_CATEGORIES categories\n",
        "if NUM_CAT > MAX_CATEGORIES:\n",
        "  THRESHOLD = (sorted_values[MAX_CATEGORIES - 1] + sorted_values[MAX_CATEGORIES]) * 0.5\n",
        "\n",
        "# first pass, assign unique ids to color categories\n",
        "id = 0\n",
        "for xcat in range(NUM_BINS):\n",
        "  for ycat, score in enumerate(hist[xcat, :]):\n",
        "    if score > THRESHOLD:\n",
        "      cat2id[(xcat, ycat)] = id\n",
        "      id2cat[id] = (xcat, ycat)\n",
        "      cat_freq[id] = score\n",
        "      id += 1\n",
        "NUM_CAT = id\n",
        "\n",
        "# second pass, mapping rare colors to frequent ones; updating frequencies\n",
        "for xcat in range(NUM_BINS):\n",
        "  for ycat, score in enumerate(hist[xcat, :]):\n",
        "    if not score > THRESHOLD:\n",
        "      closest_class = min(range(NUM_CAT),\n",
        "                          key=lambda k: (id2cat[k][0] - xcat) ** 2 + \n",
        "                          (id2cat[k][1] - ycat) ** 2)\n",
        "      cat_freq[closest_class] += score\n",
        "      cat2id[(xcat, ycat)] = closest_class\n",
        "\n",
        "# compute the weights associated with every pixel class\n",
        "weights = {k: 1. / (proba * (1. - LAMBDA) + LAMBDA / NUM_CAT) for\n",
        "                (k, proba) in cat_freq.items()}\n",
        "# normalize the weights\n",
        "normalization_factor = sum([weights[k] * cat_freq[k] for k in weights])\n",
        "weights = {k: weight / normalization_factor for k, weight in weights.items()}\n",
        "\n",
        "print(f'number of weights: {len(weights)}')\n",
        "\n",
        "categories_mean_pixels = np.zeros([NUM_CAT, 2], dtype=np.float32)\n",
        "for index in range(1, NUM_CAT):\n",
        "  xcat, ycat = id2cat[index]\n",
        "  categories_mean_pixels[index, :] = [(x_edge[xcat] + x_edge[xcat + 1]) / 2,\n",
        "                                      (y_edge[ycat] + y_edge[ycat + 1]) / 2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m32JTb_CwTxa"
      },
      "source": [
        "print('\\n =================STATS======================== \\n')\n",
        "print('id id2cat \\t cat2mean \\t cat_freq \\t\\t weight: \\n')\n",
        "\n",
        "for i in range(NUM_CAT):\n",
        "  print(i, \n",
        "    id2cat[i], \n",
        "    '\\t', \n",
        "    categories_mean_pixels[i],\n",
        "    '\\t',\n",
        "    cat_freq[i],\n",
        "    '\\t',\n",
        "    weights[i],\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCsZ2nqKxnPn"
      },
      "source": [
        "### Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfDgTLQXxoyd"
      },
      "source": [
        "hm = np.copy(hist)\n",
        "hm[hm < THRESHOLD] = 0\n",
        "logheatmap = np.log10(hm)\n",
        "extent = [x_edge[0] - 10, x_edge[-1] + 10, y_edge[0] - 10, y_edge[-1] + 10]\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.subplot(222)\n",
        "plt.imshow(logheatmap.T, extent=extent, origin='lower')\n",
        "plt.colorbar()\n",
        "plt.title(\"Frequency map (log-scale)\", fontsize = 15, color='white')\n",
        "\n",
        "plt.subplot(224)\n",
        "weights_matrix = np.zeros([NUM_BINS, NUM_BINS])\n",
        "for k in weights:\n",
        "    weights_matrix[id2cat[k]] = weights[k]\n",
        "logweights_matrix = np.log10(weights_matrix)\n",
        "plt.imshow(logweights_matrix.T, extent=extent, origin='lower')\n",
        "plt.colorbar()\n",
        "plt.title('Weight map (log-scale)', fontsize = 15, color='white')\n",
        "\n",
        "plt.subplot(221)\n",
        "color_matrix = np.ones([NUM_BINS, NUM_BINS, 3], dtype=np.float32)\n",
        "for k in weights:\n",
        "    yuv = np.zeros([1, 1, 3]) + 0.5\n",
        "    yuv[..., 1:] = categories_mean_pixels[k]\n",
        "\n",
        "    if VERBOSE:\n",
        "      print('yuv')\n",
        "      # u\n",
        "      print(np.min(yuv[..., 1]))\n",
        "      # v\n",
        "      print(np.min(yuv[..., 2]))\n",
        "      pass\n",
        "    \n",
        "    rgb = cv2.cvtColor(yuv.astype(np.float32), cv2.COLOR_YCrCb2RGB)\n",
        "    \n",
        "    if VERBOSE:\n",
        "      print('rgb')\n",
        "      # r\n",
        "      print(np.min(rgb[..., 0]))\n",
        "      # g\n",
        "      print(np.min(rgb[..., 1]))\n",
        "      # b\n",
        "      print(np.max(rgb[..., 2]))  \n",
        "      pass\n",
        "\n",
        "    color_matrix[id2cat[k][1], id2cat[k][0], :] = rgb\n",
        "plt.imshow(color_matrix, extent=extent, origin='lower')\n",
        "plt.title(\"Color map, 50% luminance\", fontsize = 15, color='white')\n",
        "\n",
        "plt.subplot(223)\n",
        "plt.imshow(-logheatmap.T, extent=extent, origin='lower')\n",
        "plt.colorbar()\n",
        "plt.title(\"Inverse-frequency map (log-scale)\", fontsize = 15, color='white')\n",
        "\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_1glwhtY-cF"
      },
      "source": [
        "### Weights save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpWHI4-uk2o6"
      },
      "source": [
        "# save the weights\n",
        "weights_np = np.float32(list(weights.values()))\n",
        "suffix = 'small'#@param{type:'string'}\n",
        "\n",
        "\n",
        "weights_file = f'sun-w-{MAX_IMG}-{NUM_CAT}-{LAMBDA}.npy'\n",
        "if len(suffix):\n",
        "  weights_file = weights_file.replace('.npy', f'-{suffix}.npy')\n",
        "np.save(os.path.join(HOME, weights_file), weights_np)\n",
        "\n",
        "# sanity checks\n",
        "print(weights_np[:5])\n",
        "\n",
        "print('min:', np.min(weights_np))\n",
        "print('max:', np.max(weights_np))\n",
        "\n",
        "assert len(weights_np) == NUM_CAT\n",
        "print(f'\\nsaved weights for {NUM_CAT} classes as: \\n{weights_file}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huz9SyPTFS2n"
      },
      "source": [
        "### Conversion/categorization functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mn4FXutzmmf"
      },
      "source": [
        "def categorize_uv_pixels(uv_px, x_edge, y_edge, cat2id):\n",
        "  u_px = uv_px[:, :, 0]\n",
        "  u_flat = np.ravel(u_px)\n",
        "\n",
        "  v_px = uv_px[:, :, 1]\n",
        "  v_flat = np.ravel(v_px)\n",
        "\n",
        "  upx_cat = np.searchsorted(x_edge[:-1], u_flat) - 1\n",
        "  vpx_cat = np.searchsorted(y_edge[:-1], v_flat) - 1\n",
        "\n",
        "  return np.reshape(np.array([cat2id[xycategories] for xycategories in\n",
        "                              zip(upx_cat, vpx_cat)]), u_px.shape)\n",
        "px_to_uvcat = lambda uvs: categorize_uv_pixels(uvs, x_edge, y_edge, cat2id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTkFR7OgKvRK"
      },
      "source": [
        " def UVpixels_from_distribution(distribution, temperature, cat_to_mean):\n",
        "  \"\"\"\n",
        "  Returns mean pixels from Npixels distributions over the color categories.\n",
        "  :param temperature: temperature of the annealed probability distribution.\n",
        "  :param distribution: matrix of size Npixels * Mpixels * n_categories.\n",
        "  \"\"\"\n",
        "  temp_distribution = np.exp(np.log(distribution + 1e-8) / temperature)\n",
        "  newshape = list(distribution.shape)\n",
        "  newshape[-1] = 1\n",
        "  temp_distribution /= np.sum(temp_distribution, axis=-1).reshape(newshape)\n",
        "\n",
        "  return np.dot(temp_distribution, cat_to_mean)\n",
        "\n",
        "uv_px_from_z = lambda z, temp: UVpixels_from_distribution(z, temp, categories_mean_pixels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYq71tDHc4Ex"
      },
      "source": [
        "## 2) Query image urls"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNq_zMWoJeoS"
      },
      "source": [
        "is_within = lambda x, range: x >= range[0] and x <= range[1]\n",
        "\n",
        "IMG_SIZE = (512, 800)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-5ZVNVj_8zA"
      },
      "source": [
        "### 2.1) flickr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIiWI73nQdNJ"
      },
      "source": [
        "#### flickr query helper and params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcp8YRG1wUe8"
      },
      "source": [
        "def query_images(image_urls, params, size, num_photos, page = 1, max_taken = u''):\n",
        "  params['page'] = str(page)\n",
        "  if len(max_taken) > 0:\n",
        "    params['max_taken_date'] = max_taken\n",
        "\n",
        "  resp = flickr.photos.search(**params)\n",
        "  assert resp.attrib['stat'] == 'ok'\n",
        "\n",
        "  kept_photos = 0\n",
        "\n",
        "  page = next(resp.iter('photos'))\n",
        "  \n",
        "  print(page.attrib)\n",
        "  \n",
        "  photos = next(page.iter('photos'))\n",
        "  for p in photos.iter('photo'):\n",
        "    if len(image_urls) >= num_photos:\n",
        "      print('got all the necessary photos')\n",
        "      break\n",
        "    \n",
        "    photo_id = p.attrib['id']\n",
        "\n",
        "    size_info = flickr.photos.getSizes(photo_id = photo_id)\n",
        "    if size_info.attrib['stat'] != u'ok':\n",
        "      continue\n",
        "    size_info = next(size_info.iter('sizes'))\n",
        "    if size_info.attrib['candownload'] != u'1':\n",
        "      continue\n",
        "    for img_size in size_info.iter('size'):\n",
        "      size_attr = img_size.attrib\n",
        "      \n",
        "      if (is_within(int(size_attr['width']), size) and is_within(int(size_attr['height']), size)) \\\n",
        "          and photo_id not in image_urls:\n",
        "          \n",
        "          image_urls[photo_id] = size_attr['source']    \n",
        "          kept_photos += 1\n",
        "          break\n",
        "\n",
        "tags = ['landscape', 'beach', 'mountains', 'nature', 'sunset', 'sunrise', 'desert']\n",
        "\n",
        "MAX_QUERY_SIZE = 4000\n",
        "PER_PAGE = 500\n",
        "\n",
        "query_params = {\n",
        "    'tags' : u','.join(tags),\n",
        "    'tag_type' : u'any',\n",
        "    'license' : u'7,9,10', # no copyright, public domain\n",
        "    'safe_search' : u'1',  # safeSearch on\n",
        "    'content_type' : u'1', # photos only\n",
        "    'media' : u'photo',\n",
        "    'per_page' : str(PER_PAGE),\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C314mXYeQnG4"
      },
      "source": [
        "#### gather urls using the query"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XcC1eOpQtHi"
      },
      "source": [
        "# init the ficker api\n",
        "import flickrapi\n",
        "\n",
        "# TODO actually load from a file \n",
        "api_key = u'dc74c8ef9aa757b0f0799d06ddaad303'\n",
        "api_secret = u'b50f2b82f2a58575'\n",
        "\n",
        "flickr = flickrapi.FlickrAPI(api_key, api_secret)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLYMy3pV3HdM"
      },
      "source": [
        "PHOTOS_TO_GET =  10000#@param {type:'integer'}\n",
        "\n",
        "num_photo_urls = 0\n",
        "id_to_url_flickr = OrderedDict()\n",
        "initial_len = 0\n",
        "\n",
        "query_results_fpath = os.path.join(DATA_PATH_FLICKR, 'id2url.pickle')\n",
        "if os.path.isfile(query_results_fpath):\n",
        "  with open(query_results_fpath, 'rb') as fp:\n",
        "    id_to_url_flickr = pickle.load(fp)\n",
        "    num_photo_urls = len(id_to_url_flickr)\n",
        "    initial_len = num_photo_urls\n",
        "\n",
        "    print(f'Already have {num_photo_urls}/{PHOTOS_TO_GET} urls')\n",
        "\n",
        "page = 1\n",
        "last_taken_date = u''\n",
        "\n",
        "while num_photo_urls < PHOTOS_TO_GET:\n",
        "  prev_len = len(id_to_url_flickr)\n",
        "  \n",
        "  query_images(id_to_url_flickr, query_params, IMG_SIZE, PHOTOS_TO_GET, \n",
        "               page, last_taken_date)\n",
        "  \n",
        "  num_photo_urls = len(id_to_url_flickr)\n",
        "  print(f'page {page}, got {num_photo_urls - prev_len} urls')\n",
        "  \n",
        "  page += 1\n",
        "  \n",
        "  # if we queried more image, restrain the query to start from the last image in our set\n",
        "  if (page - 1) % (MAX_QUERY_SIZE // PER_PAGE) == 0:\n",
        "    page = 1\n",
        "\n",
        "    last_img_id = next(reversed(id_to_url_flickr))\n",
        "\n",
        "    resp = flickr.photos.getInfo(photo_id = last_img_id)\n",
        "    assert resp.attrib['stat'] == u'ok'\n",
        "\n",
        "    dates = next(iter(resp.iter('dates')))\n",
        "    last_taken_date = dates.attrib['taken']\n",
        "\n",
        "with open(query_results_fpath, 'wb') as fp:\n",
        "  pickle.dump(id_to_url_flickr, fp)\n",
        "\n",
        "ring_bell(f'\\nGot {len(id_to_url_flickr) - initial_len} additional urls')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1hnWz4pAX70"
      },
      "source": [
        "### 2.2) SUN-local"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcmdYfHABwx9"
      },
      "source": [
        "#### Gather file-paths based on kept categories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ORqWX6tJGZu"
      },
      "source": [
        "def get_i2u_sun(id_to_url, fpaths, size, num_imgs):\n",
        "  for fp in tqdm(fpaths):\n",
        "    if len(id_to_url) >= num_imgs:\n",
        "      print('got all the necessary photos')\n",
        "      break\n",
        "    \n",
        "    try:\n",
        "      image = plt.imread(fp)\n",
        "    except:\n",
        "      continue\n",
        "    \n",
        "    image_size = image.shape[:2]\n",
        "    if len(image.shape) != 3 or image.shape[-1] != 3 or min(image_size) < size:\n",
        "      continue\n",
        "    \n",
        "    image_name, extension = os.path.split(fp)[-1].split('.')[-2:]\n",
        "    if extension.lower() not in ['jpg', 'jpeg']:\n",
        "      continue\n",
        "\n",
        "    id = image_name.replace('sun_', '')\n",
        "    assert len(id)\n",
        "    \n",
        "    if id not in id_to_url:\n",
        "      id_to_url[id] = fp\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1dV9kboAq8N"
      },
      "source": [
        "PHOTOS_TO_GET = 2000 #@param{type:'number'}\n",
        "MIN_SIZE = 260\n",
        "\n",
        "num_photo_urls = 0\n",
        "id_to_url_sun = OrderedDict()\n",
        "initial_len = 0\n",
        "query_results_fpath = os.path.join(DATA_PATH_SUN, 'id2url.pickle')\n",
        "\n",
        "if os.path.isfile(query_results_fpath):\n",
        "  with open(query_results_fpath, 'rb') as fp:\n",
        "    id_to_url_sun = pickle.load(fp)\n",
        "    num_photo_urls = len(id_to_url_sun)\n",
        "    initial_len = num_photo_urls\n",
        "\n",
        "    print(f'Already have {num_photo_urls}/{PHOTOS_TO_GET} urls')\n",
        "\n",
        "max_img_per_cat = 2 * PHOTOS_TO_GET // len(kept_cats) + 1\n",
        "\n",
        "iteration = 1\n",
        "while num_photo_urls < PHOTOS_TO_GET:\n",
        "  prev_len = len(id_to_url_sun)\n",
        "  \n",
        "  print(f'iteration {iteration}:')\n",
        "  print(f'max images per category: {max_img_per_cat}')\n",
        "  \n",
        "  fpaths = query_sun_filepaths(DATA_PATH_SUN, kept_cats, max_img_per_cat)\n",
        "  get_i2u_sun(id_to_url_sun, fpaths, MIN_SIZE, PHOTOS_TO_GET)\n",
        "\n",
        "  num_photo_urls = len(id_to_url_sun)\n",
        "  print(f'got {num_photo_urls - prev_len} urls')\n",
        "  \n",
        "  max_img_per_cat += 3\n",
        "  iteration += 1\n",
        "\n",
        "with open(query_results_fpath, 'wb') as fp:\n",
        "  pickle.dump(id_to_url_sun, fp)\n",
        "ring_bell(f'\\nGot {len(id_to_url_sun) - initial_len} additional urls')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpgX9lanbA0V"
      },
      "source": [
        "# TODO TEMP DELETE\n",
        "\n",
        "id_to_url_sun = OrderedDict()\n",
        "query_results_fpath = os.path.join(DATA_PATH_SUN, 'id2url.pickle')\n",
        "fpaths = query_sun_filepaths(DATA_PATH_SUN, kept_cats, max_img_per_cat)\n",
        "\n",
        "MIN_SIZE = 256\n",
        "get_i2u_sun(id_to_url_sun, fpaths, MIN_SIZE, 1500)\n",
        "\n",
        "with open(query_results_fpath, 'wb') as fp:\n",
        "  pickle.dump(id_to_url_sun, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAkeWF6sxloy"
      },
      "source": [
        "## 3) Download and process images\n",
        "\n",
        "- Num img to get: in paper was ~14k train / ~3k test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KEUjE_NdI_n"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h19ZcomEZg07"
      },
      "source": [
        "def download_process_images(image_urls, img_dir, aug_processors, blacklist, download=True):\n",
        "  print(f'Max images to get: {len(image_urls)}\\n')\n",
        "  \n",
        "  if download:\n",
        "    print(f'starting download...')\n",
        "  else:\n",
        "    print(f'processing local images...')\n",
        "\n",
        "  os.makedirs(img_dir, exist_ok=True)\n",
        "\n",
        "  uid_to_path = OrderedDict()\n",
        "\n",
        "  MAX_DL_SIZE_B = 5e10\n",
        "  total_size_b = 0\n",
        "  num_imgs_dld = 0\n",
        "\n",
        "  for u in tqdm(image_urls):\n",
        "    id = u[0]\n",
        "    url = u[-1]\n",
        "\n",
        "    if download:\n",
        "      fpath = os.path.join(img_dir, f'{id}.jpg')\n",
        "    else:\n",
        "      fpath = url\n",
        "\n",
        "    assert fpath is not None\n",
        "\n",
        "    if fpath in blacklist:\n",
        "      if VERBOSE:\n",
        "        print(f'skipping blacklisted file: {fpath}...')\n",
        "      continue\n",
        "\n",
        "    if not os.path.isfile(fpath):\n",
        "      if download:\n",
        "        ! curl -s {url} > {fpath} \n",
        "        if VERBOSE:\n",
        "          print(f'could not find file {fpath}, downloading...')\n",
        "      else:\n",
        "        continue\n",
        "    \n",
        "    try: \n",
        "      dl_size = os.path.getsize(fpath)\n",
        "\n",
        "      image = plt.imread(fpath)\n",
        "\n",
        "      if download:\n",
        "        num_ch = len(image.shape)\n",
        "        if num_ch != 3 or (num_ch == 3 and image.shape[-1] != 3):\n",
        "          blacklist.add(fpath)\n",
        "          \n",
        "          if VERBOSE:\n",
        "            print(f'skipping non rgb image {fpath}...')\n",
        "            #print(image.shape)\n",
        "          continue\n",
        "\n",
        "      if len(aug_processors) == 0:\n",
        "        uid_to_path[id] = fpath\n",
        "\n",
        "      for proc in aug_processors:\n",
        "        n_id, n_path = proc(id, fpath)\n",
        "        if n_id is not None and n_path is not None:\n",
        "          uid_to_path[n_id] = n_path\n",
        "\n",
        "    except OSError:\n",
        "      continue\n",
        "\n",
        "    total_size_b += dl_size\n",
        "    num_imgs_dld += dl_size > 0\n",
        "\n",
        "    if total_size_b >= MAX_DL_SIZE_B and download:\n",
        "      print('exceeded max download size')\n",
        "      break\n",
        "\n",
        "  id_to_uid = dict(enumerate(uid_to_path.keys()))\n",
        "  size_factor = max(len(aug_processors), 1)\n",
        "  approx_size_k = int(total_size_b/1000 * size_factor)\n",
        "  \n",
        "  if download:\n",
        "    print(f'download complete, got ~{approx_size_k}K')\n",
        "\n",
        "  for img_path in blacklist:\n",
        "    try:\n",
        "      os.remove(img_path)\n",
        "    except OSError:\n",
        "      continue\n",
        "  \n",
        "  return uid_to_path, id_to_uid\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTc8zqB6KWRk"
      },
      "source": [
        "### Image processors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQ-keM5YsO4b"
      },
      "source": [
        "class ImageProcessor():\n",
        "  def __init__(self, transform, label=''):\n",
        "    self.transform = transform\n",
        "    self.label = label\n",
        "\n",
        "  def __call__(self, img_id, img_path):\n",
        "    \n",
        "    assert img_id is not None and img_path is not None, 'No id or path'\n",
        "    assert os.path.getsize(img_path) > 0, 'empty file'\n",
        "\n",
        "    try:\n",
        "      image = Image.open(img_path)\n",
        "      if img_path.split('.')[-1].lower() in ('jpg', 'jpeg', 'jpe', \n",
        "                                             'jif', 'jfif', 'jfi', 'tif', 'tiff'):\n",
        "        exif_data = image._getexif()\n",
        "\n",
        "    except AttributeError as ae:\n",
        "      #print(f'{img_path}:{ae}')\n",
        "      return None, None\n",
        "\n",
        "    except (IOError, ValueError) as ioe:\n",
        "      #print(ioe)\n",
        "      return None, None\n",
        "    \n",
        "    new_id = img_id\n",
        "    if len(self.label):\n",
        "      new_id += f'-{self.label}'\n",
        "    new_path = img_path.replace(img_id, new_id)\n",
        "    \n",
        "    if not os.path.isfile(new_path):\n",
        "      transformed = self.transform(image)\n",
        "      transformed.save(new_path)\n",
        "    \n",
        "    if os.path.getsize(new_path) > 0:\n",
        "      return new_id, new_path\n",
        "    \n",
        "    return None, None\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIDS3RSk0HTK"
      },
      "source": [
        "def add_gaussian_noise(img, mean = 0, std = 1):\n",
        "\n",
        "  img_np = np.array(img)\n",
        "  noise = np.random.normal(mean, std, img_np.shape)\n",
        "  noisy = img + noise\n",
        "  noisy = np.uint8((noisy / noisy.max()) * 255.)\n",
        "  \n",
        "  return Image.fromarray(noisy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEyNV3u_hidy"
      },
      "source": [
        "SQ_SIZE = 256\n",
        "\n",
        "random_crop256 = tvt.Compose([\n",
        "    tvt.Resize(SQ_SIZE, interpolation=Image.LANCZOS),\n",
        "    tvt.RandomCrop(size=SQ_SIZE)\n",
        "])\n",
        "base_transform = ImageProcessor(random_crop256)\n",
        "proc_rc1 = ImageProcessor(random_crop256, 'rc1')\n",
        "proc_rc2 = ImageProcessor(random_crop256, 'rc2')\n",
        "proc_rc3 = ImageProcessor(random_crop256, 'rc3')\n",
        "\n",
        "flipX = tvt.Compose([\n",
        "    random_crop256,\n",
        "    tvt.RandomHorizontalFlip(p = 1.)\n",
        "])\n",
        "proc_flip_x = ImageProcessor(flipX, 'flip-x')\n",
        "\n",
        "flipY = tvt.Compose([\n",
        "    random_crop256,\n",
        "    tvt.RandomVerticalFlip(p = 1.)\n",
        "])\n",
        "proc_flip_y = ImageProcessor(flipY, 'flip-y')\n",
        "\n",
        "noise_l = lambda img: add_gaussian_noise(img, mean=0.1, std=0.05)\n",
        "add_noise = tvt.Compose([\n",
        "    random_crop256,\n",
        "    tvt.Lambda(noise_l)\n",
        "])\n",
        "proc_noise = ImageProcessor(add_noise, 'n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BskbhNsYKaBO"
      },
      "source": [
        "### Train/test/val split the urls and download/process the images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_IM3PMRJ0NN"
      },
      "source": [
        "VERBOSE = False\n",
        "# either use flickr or SUN\n",
        "USE_FLICKR = False #@param{type:'boolean'}\n",
        "\n",
        "PHOTOS_TO_USE =  2000#@param{type:'integer'}\n",
        "\n",
        "id_to_url = id_to_url_flickr if USE_FLICKR else id_to_url_sun \n",
        "\n",
        "train_imgs, test_imgs = train_test_split(list(id_to_url.items())[:PHOTOS_TO_USE], \n",
        "                                         test_size = 0.1, shuffle=False)\n",
        "train_imgs, valid_imgs = train_test_split(train_imgs, \n",
        "                                          test_size = 0.1, shuffle=False)\n",
        "\n",
        "print(f'train samples orig.:\\t {len(train_imgs)}')\n",
        "print(f'val. samples orig.:\\t {len(valid_imgs)}')\n",
        "print(f'test samples orig.:\\t {len(test_imgs)}')\n",
        "print('\\n')\n",
        "\n",
        "# ====================================================\n",
        "\n",
        "data_path = DATA_PATH_FLICKR if USE_FLICKR else DATA_PATH_SUN\n",
        "IMAGE_PATH = os.path.join(data_path, 'images')\n",
        "\n",
        "BLACKLIST = set()\n",
        "blacklist_path = os.path.join(data_path, 'blacklist.pickle')\n",
        "if os.path.isfile(blacklist_path):\n",
        "  with open(blacklist_path, 'rb') as fp:\n",
        "    initial_blacklist = pickle.load(fp)\n",
        "    BLACKLIST.update(initial_blacklist)\n",
        "\n",
        "start_time = time.time()\n",
        "train_procs = [proc_rc1, proc_rc2, proc_flip_x, proc_noise]\n",
        "\n",
        "u2p_train, i2u_train = download_process_images(\n",
        "    train_imgs, \n",
        "    IMAGE_PATH,\n",
        "    train_procs,\n",
        "    BLACKLIST,\n",
        "    download = USE_FLICKR)\n",
        "\n",
        "u2p_val, i2u_val = download_process_images(\n",
        "    valid_imgs, \n",
        "    IMAGE_PATH,\n",
        "    train_procs,\n",
        "    BLACKLIST,\n",
        "    download = USE_FLICKR)\n",
        "\n",
        "test_procs = []\n",
        "\n",
        "u2p_test, i2u_test = download_process_images(\n",
        "    test_imgs, \n",
        "    IMAGE_PATH,\n",
        "    test_procs,\n",
        "    BLACKLIST,\n",
        "    download = USE_FLICKR)\n",
        "\n",
        "with open(blacklist_path, 'wb') as fp:\n",
        "  pickle.dump(BLACKLIST, fp)\n",
        "\n",
        "print(f'\\ntrain samples aug.:\\t {len(i2u_train)}')\n",
        "print(f'val. samples aug.:\\t {len(i2u_val)}')\n",
        "print(f'test samples aug.:\\t {len(i2u_test)}')\n",
        "\n",
        "stat_str = 'local data processed' if not USE_FLICKR else \\\n",
        "  'data downloaded and processed'\n",
        "stat_str = f'\\n{stat_str} \\n\\ntook {time.time()-start_time:.2f}s\\n'\n",
        "\n",
        "ring_bell(stat_str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP5ki_kDxbWP"
      },
      "source": [
        "### Sanity check and cleanup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqQv9d1-1TPK"
      },
      "source": [
        "num_bad_images = !find {IMAGE_PATH} -type f -size 0 | wc -l\n",
        "num_bad_images = int(num_bad_images[0])\n",
        "\n",
        "assert num_bad_images == 0, f'found {num_bad_images} bad images'\n",
        "\n",
        "PURGE = True #@param{type:'boolean'}\n",
        "if PURGE and num_bad_images:\n",
        "  ! find {IMAGE_PATH} -type f -size 0 -delete\n",
        "  print('deleted bad images')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS0KJaHeohd7"
      },
      "source": [
        "### Custom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_W_c7aJy5LpG"
      },
      "source": [
        "class ColorizeDataset(Dataset):\n",
        "  def __init__(self, uid2path, id2uid, cat_fn = None, is_test = False, resize = None):\n",
        "    super().__init__()\n",
        "\n",
        "    assert len(uid2path) == len(id2uid), 'dataset maps length should match'\n",
        "    \n",
        "    self.uid2path = uid2path\n",
        "    self.id2uid = id2uid\n",
        "    self.cat_fn = cat_fn\n",
        "    self.is_test = is_test\n",
        "\n",
        "    self.resize = resize\n",
        "  \n",
        "  def __getitem__(self, id):\n",
        "    if id not in self.id2uid.keys():\n",
        "      raise IndexError\n",
        "\n",
        "    path = self.uid2path[self.id2uid[id]]\n",
        "    image = cv2.imread(path)\n",
        "\n",
        "    assert image is not None\n",
        "\n",
        "    if self.resize is not None:\n",
        "      image = cv2.resize(image, (self.resize, self.resize), interpolation = cv2.INTER_LANCZOS4)\n",
        "\n",
        "    # normalize image prior to conversion from BGR\n",
        "    image = np.float32(image * 1./255)\n",
        "    # convert image to yuv in place, \n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2YCrCb)\n",
        "\n",
        "    # input, luminance channel\n",
        "    luma = torch.from_numpy(image[..., 0]) \n",
        "    # output, chroma channels, binned\n",
        "    chroma = None\n",
        "    if self.cat_fn is not None:\n",
        "      chroma = self.cat_fn(image[..., 1:])\n",
        "      chroma = torch.from_numpy(chroma)\n",
        "\n",
        "    # we'll only test the grayscale image and need the path for the original file\n",
        "    if self.is_test:\n",
        "      return luma, path\n",
        "\n",
        "    return luma, chroma, path\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.id2uid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUEFPisBPKj4"
      },
      "source": [
        "# Network architecture\n",
        "\n",
        "implemented from https://arxiv.org/pdf/1811.03120.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piyyaSU6giHn"
      },
      "source": [
        "### Cells"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7VUKcxFVDAg"
      },
      "source": [
        "class DownConvCell(nn.Module):\n",
        "  def __init__(self, ich, och):\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(ich, och, 3, 1, 1)\n",
        "    self.conv2 = nn.Conv2d(och, och, 3, 1, 1)\n",
        "    \n",
        "    self.nl = nn.ReLU()\n",
        "    \n",
        "    self.mp = nn.MaxPool2d(2, 2)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    x = self.nl(self.conv1(x))\n",
        "    x = self.nl(self.conv2(x))\n",
        "\n",
        "    x = self.mp(x)\n",
        "    \n",
        "    return F.layer_norm(x, x.shape[1:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qn4Q6XDujXOw"
      },
      "source": [
        "class UpConvCell(nn.Module):\n",
        "  def __init__(self, ich, och):\n",
        "    super().__init__()\n",
        "\n",
        "    self.upconv = nn.ConvTranspose2d(ich, och, 4, 2, 1)\n",
        "    self.conv1 = nn.Conv2d(och, och, 3, 1, 1)\n",
        "    self.conv2 = nn.Conv2d(och, och, 3, 1, 1)\n",
        "    \n",
        "    self.nl = nn.ReLU()\n",
        "\n",
        "  def __call__(self, x):\n",
        "    x = self.nl(self.upconv(x))\n",
        "    #print(x.shape)\n",
        "\n",
        "    x = self.nl(self.conv1(x))\n",
        "    #print(x.shape)\n",
        "    \n",
        "    x = self.nl(self.conv2(x))\n",
        "    #print(x.shape)\n",
        "    \n",
        "    return F.layer_norm(x, x.shape[1:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQAbn9cYl_vW"
      },
      "source": [
        "class OutputCell(nn.Module):\n",
        "  def __init__(self, ich, och):\n",
        "    super().__init__()\n",
        "\n",
        "    self.upconv = nn.ConvTranspose2d(ich, ich, 4, 2, 1)\n",
        "    self.conv1 = nn.Conv2d(ich, ich, 3, 1, 1)\n",
        "    self.conv2 = nn.Conv2d(ich, och, 3, 1, 1)\n",
        "    \n",
        "    self.nl = nn.ReLU()\n",
        "\n",
        "  def __call__(self, x):\n",
        "    x = self.nl(self.upconv(x))\n",
        "    x = self.nl(self.conv1(x))\n",
        "    \n",
        "    return self.conv2(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV9woQpX09FB"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iw8hvCZFm8Yc"
      },
      "source": [
        "class ColorUnet(nn.Module):\n",
        "  def __init__(self, input_ch, num_cls):\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_ch = 1\n",
        "    self.num_cls = num_cls\n",
        "    \n",
        "    self.downConv1 = DownConvCell(input_ch, num_cls // 2)\n",
        "    self.downConv2 = DownConvCell(num_cls // 2, num_cls)\n",
        "    self.downConv3 = DownConvCell(num_cls, 2 * num_cls)\n",
        "\n",
        "    self.upConv1 = UpConvCell(2 * num_cls, num_cls)\n",
        "    self.upConv2 = UpConvCell(2 * num_cls, num_cls // 2)\n",
        "\n",
        "    self.outCell = OutputCell(num_cls, num_cls)\n",
        "  \n",
        "  def __call__(self, x):\n",
        "\n",
        "    # input shape is (B, H, W)    \n",
        "    # reshape it as (B, C, H, W)\n",
        "    x.unsqueeze_(1)\n",
        "    #print(x.shape)\n",
        "\n",
        "    # save down conv cell 1 output\n",
        "    x_1 = self.downConv1(x)\n",
        "    #print('x1s', x_1.shape)\n",
        "\n",
        "    # save down conv cell 2 output\n",
        "    x_2 = self.downConv2(x_1)\n",
        "    #print('x2s', x_2.shape)\n",
        "\n",
        "    # apply down conv 3 and up conv 1\n",
        "    x = self.downConv3(x_2)\n",
        "    #print('xs dc3', x.shape)\n",
        "    \n",
        "    x = self.upConv1(x)\n",
        "    #print('xs uc1', x.shape)\n",
        "\n",
        "    # concat channelsand apply up conv 2\n",
        "    x = torch.cat((x, x_2), axis = 1)\n",
        "    #print('xs pre-uc2 cat', x.shape)\n",
        "    \n",
        "    x = self.upConv2(x)\n",
        "    #print('xs uc2', x.shape)\n",
        "\n",
        "    # concat channels and apply output layer\n",
        "    x = torch.cat((x, x_1), axis = 1)\n",
        "    #print('xs pre-out cat', x.shape)\n",
        "    \n",
        "    x = self.outCell(x)\n",
        "    #print('xs out', x.shape)\n",
        "\n",
        "    # output is (B, C, H, W)\n",
        "    # reshape X to be (B, H, W, C)\n",
        "    x = x.permute(0, 2, 3, 1)\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1kJYDFHfNBr"
      },
      "source": [
        "# Train/test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bskRAaXCSrI"
      },
      "source": [
        "### Helpers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQY9liPiCUy9"
      },
      "source": [
        "def train_step(model, criterion, optimizer, loader, device):\n",
        "  epoch_loss = 0\n",
        "  n_iter = 0\n",
        "  \n",
        "  model.train()\n",
        "  for i, data in tqdm(enumerate(loader), desc='train', total=len(loader)):\n",
        "    luma = data[0].float().to(device)\n",
        "    chroma_cat = data[1].long().to(device)\n",
        "\n",
        "    out = model(luma)\n",
        "\n",
        "    # input / target shape is (B, H, W, C), loss expects (B, C, ...)\n",
        "    loss = criterion(out.permute(0, 3, 1, 2), chroma_cat.squeeze())\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      optimizer.step()\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "      n_iter += 1\n",
        "  \n",
        "  return epoch_loss/n_iter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P78XeGdEHPeB"
      },
      "source": [
        "def valid_step(model, criterion, loader, device):\n",
        "  epoch_loss = 0\n",
        "  n_iter = 0\n",
        "  \n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for i, data in tqdm(enumerate(loader), desc='validate', total=len(loader)):\n",
        "      luma = data[0].float().to(device)\n",
        "      chroma_cat = data[1].long().to(device)\n",
        "\n",
        "      out = model(luma)\n",
        "      \n",
        "      # input / target shape is (B, H, W, C), loss expects (B, C, ...)\n",
        "      loss = criterion(out.permute(0, 3, 1, 2), chroma_cat.squeeze())\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "      n_iter += 1\n",
        "  \n",
        "  return epoch_loss/n_iter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36yxXHVPhr1E"
      },
      "source": [
        "def gen_model_name(ds_size, num_epochs, prefix='m', suffix=''):\n",
        "  num_dec = 2\n",
        "  if ds_size >= 1000:\n",
        "    num_dec = 0\n",
        "  \n",
        "  name = [prefix, f'{num_epochs}e', f'{ds_size/1000:.{num_dec}f}k']\n",
        "  if len(suffix):\n",
        "    name += [suffix]\n",
        "  \n",
        "  return '-'.join(name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRvuALNsCQfw"
      },
      "source": [
        "## Dataloader init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCPdGjY4DBYm"
      },
      "source": [
        "BATCH_SIZE = 48\n",
        "\n",
        "dl_args = {\n",
        "    'batch_size': BATCH_SIZE, \n",
        "    'shuffle': True,\n",
        "    'num_workers': 3,\n",
        "    'pin_memory': CUDA,\n",
        "    'drop_last': True\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jkb3AoOlfPU9"
      },
      "source": [
        "train_dataset = ColorizeDataset(u2p_train, i2u_train, cat_fn=px_to_uvcat)\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    **dl_args\n",
        ")\n",
        "\n",
        "val_dataset = ColorizeDataset(u2p_val, i2u_val, cat_fn=px_to_uvcat)\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    **dl_args\n",
        ")\n",
        "\n",
        "test_dataset = ColorizeDataset(u2p_test, i2u_test, is_test=True, resize=SQ_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXRPIeGoEXSL"
      },
      "source": [
        "## Sanity checks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iHcxBTlUvIO"
      },
      "source": [
        "path = u2p_test[i2u_test[0]]\n",
        "image = cv2.imread(path)\n",
        "print(f'read img from {path}')\n",
        "print(image.shape)\n",
        "print(type(image))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLSeBUqcDY9R"
      },
      "source": [
        "Ys, UV_cats = next(iter(train_loader))\n",
        "\n",
        "print(Ys.shape)\n",
        "print(UV_cats.shape)\n",
        "\n",
        "#print(Ys[0][:3, :3])\n",
        "print(torch.min(Ys[0]).item())\n",
        "print(torch.max(Ys[0]).item())\n",
        "\n",
        "#print(UV_cats[0][:3, :3])\n",
        "\n",
        "print(torch.min(UV_cats[0]).item())\n",
        "print(torch.max(UV_cats[0]).item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65NGdoKM9uRt"
      },
      "source": [
        "luma, p = test_dataset[0]\n",
        "\n",
        "print(luma.shape)\n",
        "\n",
        "print(luma.dtype)\n",
        "print(luma.min().item())\n",
        "print(luma.max().item())\n",
        "\n",
        "print(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aovZJaBVe-ZY"
      },
      "source": [
        "test_model = ColorUnet(1, 32)\n",
        "test_model.eval()\n",
        "\n",
        "luma, _ = test_dataset[0]\n",
        "\n",
        "luma.unsqueeze_(0)\n",
        "print(luma.shape)\n",
        "\n",
        "y = test_model(luma)\n",
        "print(y.shape)\n",
        "\n",
        "del test_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXT0O45-Caon"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4f_cYE2QcHP"
      },
      "source": [
        "### Model init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEn5OeGKChcF"
      },
      "source": [
        "MODELS_PATH = os.path.join(HOME, 'models')\n",
        "os.makedirs(MODELS_PATH, exist_ok = True)\n",
        "\n",
        "num_epochs =  25#@param{type:'integer'}\n",
        "lr =  1e-5 #@param{type:'number'}\n",
        "log_every = 1 #@param{type:'integer'}\n",
        "\n",
        "# early stopping\n",
        "stop_early = True #@param{type:'boolean'}\n",
        "patience = 2 #@param{type:'integer'}\n",
        "delta_tol =  1e-3#@param{type:'number'}\n",
        "\n",
        "# model init\n",
        "input_ch = 1 #@param{type:'integer'}\n",
        "model = ColorUnet(input_ch, NUM_CAT).to(DEVICE)\n",
        "optimizer = optim.Adam(model.parameters(), lr)\n",
        "\n",
        "rebalance = False #@param{type:'boolean'}\n",
        "# load weights for rebalancing\n",
        "weights_fp = '100-6-0.4-small'#@param{type:'string'}\n",
        "\n",
        "if rebalance and len(weights_fp):\n",
        "  weights = np.load(os.path.join(HOME, f'sun-w-{weights_fp}.npy'), allow_pickle=True)\n",
        "  weights = torch.from_numpy(weights).float().to(DEVICE)\n",
        "  criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "  print(f'using {weights_fp} weights for rebalancing')\n",
        "else:\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "suffix = 'sun-6cls-no-reb'#@param{type:'string'}\n",
        "model_name = gen_model_name(len(train_dataset), num_epochs, suffix=suffix)\n",
        "print(f'model name initial: {model_name}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPpCfUf4QeHd"
      },
      "source": [
        "### Actual training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iMTnnBRFqmJ"
      },
      "source": [
        "train_loss = []\n",
        "val_loss = []\n",
        "\n",
        "best_loss = 0\n",
        "num_bad_epochs = 0\n",
        "for e in range(num_epochs):\n",
        "  print(f'epoch {e+1}/{num_epochs}:\\n')\n",
        "  epoch_start = time.time()\n",
        "  \n",
        "  t_loss = train_step(model, criterion, optimizer, train_loader, DEVICE)\n",
        "  v_loss = valid_step(model, criterion, val_loader, DEVICE)\n",
        "\n",
        "  train_loss.append(t_loss)\n",
        "  val_loss.append(v_loss)\n",
        "\n",
        "  found_best_loss = False\n",
        "  if e == 0:\n",
        "    best_loss = v_loss\n",
        "  else:\n",
        "    if v_loss <= best_loss:\n",
        "      best_loss = v_loss\n",
        "      num_bad_epochs = 0\n",
        "      found_best_loss = True\n",
        "    elif v_loss - best_loss > delta_tol:\n",
        "      num_bad_epochs += 1\n",
        "\n",
        "  if e % log_every == 0:\n",
        "    print(f'train. loss:\\t{t_loss}')\n",
        "    print(f'valid. loss:\\t{v_loss}')\n",
        "    if found_best_loss:\n",
        "      print('encountered lowest val. loss')\n",
        "\n",
        "    print(f'\\ntook {(time.time() - epoch_start):.3f}s')\n",
        "    print('---------------------------------\\n')\n",
        "  \n",
        "  if num_bad_epochs >= patience and stop_early:\n",
        "    print(f'early stopping after {patience} bad epochs. trained for {e+1} epochs')\n",
        "    model_name = model_name.replace(f'{num_epochs}e-', f'{e+1}e-')\n",
        "    break\n",
        "\n",
        "model_path = os.path.join(MODELS_PATH, f'{model_name}.pt')\n",
        "print(f'model will be saved at: {model_path}')\n",
        "torch.save(model.state_dict(), model_path)\n",
        "\n",
        "ring_bell('training over')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ss49w_UOq5q5"
      },
      "source": [
        "plt.plot(train_loss, label='train', color='red')\n",
        "plt.plot(val_loss, label='valid.', color='cyan')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "680wB6dLCfKd"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zP9NLoaULRIa"
      },
      "source": [
        "### Optional. Load model state from disk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_RejGsFLF5T"
      },
      "source": [
        "model_name = ''#@param{type:'string'}\n",
        "m_path = os.path.join(MODELS_PATH, f'{model_name}.pt')\n",
        "\n",
        "model_state = None\n",
        "try:\n",
        "  model_state = torch.load(m_path)\n",
        "\n",
        "  model.load_state_dict(model_state)\n",
        "  print(f'loaded model state for predict: {m_path}')\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "  model_state = None\n",
        "  print(f'could not load model state from {m_path}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be-COXDcLWI1"
      },
      "source": [
        "### Prediction for TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5Pw_9gDcjN9"
      },
      "source": [
        "VERBOSE = False #@param{type:'boolean'}\n",
        "\n",
        "PRED_PATH = os.path.join(HOME, 'predict', model_name)\n",
        "MAX_SAVED =  25#@param{type:'integer'}\n",
        "MAX_SAVED = min(MAX_SAVED, len(train_dataset))\n",
        "\n",
        "print(f'predicted images will be saved at {PRED_PATH}')\n",
        "PURGE = True #@param{type:'boolean'}\n",
        "if PURGE:\n",
        "  print('deleted prev. predictions')\n",
        "  ! rm -rf {PRED_PATH}\n",
        "os.makedirs(PRED_PATH, exist_ok=True)\n",
        "\n",
        "temps = [0.05, 0.1, 0.3, 0.6, 1]\n",
        "print('starting prediction...\\n')\n",
        "\n",
        "model.eval()\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "  for i, data in tqdm(enumerate(test_dataset), desc='predict', total=MAX_SAVED):\n",
        "    if i >= MAX_SAVED:\n",
        "      break\n",
        "\n",
        "    Y, p = data\n",
        "    \n",
        "    Y = Y.float().to(DEVICE).unsqueeze(0)\n",
        "    Z = torch.softmax(model(Y), -1).cpu().numpy()\n",
        "    \n",
        "    if VERBOSE:\n",
        "      print('\\nY')\n",
        "      #print(Y.shape)\n",
        "      #print(Y)\n",
        "      print(Y.min().item())\n",
        "      print(Y.max().item())\n",
        "\n",
        "      print('\\nZ')\n",
        "      #print(Z)\n",
        "      print(np.min(Z))\n",
        "      print(np.max(Z))\n",
        "      #print(Z.shape)\n",
        "      pass\n",
        "\n",
        "    orig_img = cv2.imread(p)\n",
        "    assert orig_img is not None, f'Could not find original image at: {p}'\n",
        "\n",
        "    # save a thumbnail of the ground truth image\n",
        "    new_size = (orig_img.shape[1]//2, orig_img.shape[0]//2)\n",
        "    orig_img_thumb = cv2.resize(orig_img, new_size)\n",
        "    orig_id = os.path.split(p)[-1].replace('.jpg', '-gt-thumb.jpg')\n",
        "    cv2.imwrite(os.path.join(PRED_PATH, orig_id), orig_img_thumb)\n",
        "\n",
        "    # normalize gt image prior to conversion from BGR\n",
        "    orig_img = np.float32(orig_img * 1./255)\n",
        "    # convert image to yuv in place, \n",
        "    orig_img = cv2.cvtColor(orig_img, cv2.COLOR_BGR2YCrCb)\n",
        "\n",
        "    # save the luminance of the original image\n",
        "    Y_orig = orig_img[..., 0]\n",
        "\n",
        "    # save the grayscale image as well\n",
        "    gray_id = os.path.split(p)[-1].replace('.jpg', '-gray.jpg')\n",
        "    cv2.imwrite(os.path.join(PRED_PATH, gray_id), Y_orig * 255)\n",
        "\n",
        "    for t in temps:\n",
        "      UV = uv_px_from_z(Z, t).squeeze()\n",
        "\n",
        "      # sanity checks\n",
        "      if VERBOSE:\n",
        "        #print(UV)\n",
        "        print(f'\\ntemp: {t}')\n",
        "        print('-------------')\n",
        "\n",
        "        print('\\nU:')\n",
        "        print(np.min(UV[..., 0]))\n",
        "        print(np.max(UV[..., 0]))\n",
        "        print('\\nV:')\n",
        "        print(np.min(UV[..., 1]))\n",
        "        print(np.max(UV[..., 1]))\n",
        "\n",
        "        print('\\nUV shape:', UV.shape)\n",
        "        pass\n",
        "      \n",
        "      UV = cv2.resize(UV, Y_orig.shape[:2][::-1], interpolation = cv2.INTER_LANCZOS4)\n",
        "      \n",
        "      pred_img = np.concatenate((np.expand_dims(Y_orig, -1), UV), axis=-1)\n",
        "      \n",
        "      if VERBOSE:\n",
        "        print('\\nY after resize and concat')\n",
        "        print(np.min(pred_img[..., 0]))\n",
        "        print(np.max(pred_img[..., 0]))\n",
        "\n",
        "        print('\\nU after resize and concat')\n",
        "        print(np.min(pred_img[..., 1]))\n",
        "        print(np.max(pred_img[..., 1]))\n",
        "\n",
        "        print('\\nV after resize and concat')\n",
        "        print(np.min(pred_img[..., 2]))\n",
        "        print(np.max(pred_img[..., 2]))\n",
        "        pass\n",
        "\n",
        "      pred_img_cv = cv2.cvtColor(pred_img, cv2.COLOR_YCrCb2RGB)\n",
        "      pred_img_cv = np.clip(pred_img_cv, 0, 1) * 255\n",
        "\n",
        "      if VERBOSE:\n",
        "        print('\\n\\n')\n",
        "\n",
        "        print('R after rgb conversion')\n",
        "        print(np.min(pred_img_cv[..., 0]))\n",
        "        print(np.max(pred_img_cv[..., 0]))\n",
        "\n",
        "        print('G after rgb conversion')\n",
        "        print(np.min(pred_img_cv[..., 1]))\n",
        "        print(np.max(pred_img_cv[..., 1]))\n",
        "\n",
        "        print('B after rgb conversion')\n",
        "        print(np.min(pred_img_cv[..., 2]))\n",
        "        print(np.max(pred_img_cv[..., 2]))\n",
        "        pass\n",
        "\n",
        "      fname = os.path.split(p)[-1].replace('.jpg', f'-pred-{t}.jpg')\n",
        "      fname = os.path.join(PRED_PATH, f'{fname}')\n",
        "\n",
        "      cv2.imwrite(fname, pred_img_cv)\n",
        "\n",
        "ring_bell(f'\\nprediction done for {i} images')\n",
        "print(f'took {(time.time() - start_time):.2f}s')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-a9TDpAgFVC"
      },
      "source": [
        "### Prediction for TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6F3Qm_qgFVD"
      },
      "source": [
        "VERBOSE = False #@param{type:'boolean'}\n",
        "\n",
        "PRED_PATH = os.path.join(HOME, 'predict-TRAIN', model_name)\n",
        "MAX_SAVED =  25#@param{type:'integer'}\n",
        "MAX_SAVED = min(MAX_SAVED, len(train_dataset))\n",
        "\n",
        "print(f'predicted images will be saved at {PRED_PATH}')\n",
        "PURGE = True #@param{type:'boolean'}\n",
        "if PURGE:\n",
        "  print('deleted prev. predictions')\n",
        "  ! rm -rf {PRED_PATH}\n",
        "os.makedirs(PRED_PATH, exist_ok=True)\n",
        "\n",
        "temps = [0.3, 1]\n",
        "print('starting prediction...\\n')\n",
        "\n",
        "model.eval()\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "  for i, data in tqdm(enumerate(train_dataset), desc='predict-T', total=MAX_SAVED):\n",
        "    if i >= MAX_SAVED:\n",
        "      break\n",
        "\n",
        "    Y, UV, p = data\n",
        "    orig_img = cv2.imread(p)\n",
        "\n",
        "    # save the orig image\n",
        "    orig_id = os.path.split(p)[-1].replace('.jpg', '-gt-thumb.jpg')\n",
        "    cv2.imwrite(os.path.join(PRED_PATH, orig_id), orig_img)\n",
        "\n",
        "    # normalize gt image prior to conversion from BGR\n",
        "    orig_img = np.float32(orig_img * 1./255)\n",
        "    # convert image to yuv in place, \n",
        "    orig_img = cv2.cvtColor(orig_img, cv2.COLOR_BGR2YCrCb)\n",
        "\n",
        "    # save the luminance of the original image\n",
        "    Y_orig = orig_img[..., 0]\n",
        "\n",
        "    # save the grayscale image as well\n",
        "    gray_id = os.path.split(p)[-1].replace('.jpg', '-gray.jpg')\n",
        "    cv2.imwrite(os.path.join(PRED_PATH, gray_id), Y_orig * 255)\n",
        "\n",
        "    Y = Y.float().to(DEVICE).unsqueeze(0)\n",
        "    Z = torch.softmax(model(Y), -1).cpu().numpy()\n",
        "\n",
        "    for t in temps:\n",
        "      UV = uv_px_from_z(Z, t).squeeze()\n",
        "      \n",
        "      pred_img = np.concatenate((np.expand_dims(Y_orig, -1), UV), axis=-1)\n",
        "\n",
        "      pred_img_cv = cv2.cvtColor(pred_img, cv2.COLOR_YCrCb2RGB)\n",
        "      pred_img_cv = np.clip(pred_img_cv, 0, 1) * 255\n",
        "\n",
        "      fname = os.path.split(p)[-1].replace('.jpg', f'-pred-{t}.jpg')\n",
        "      fname = os.path.join(PRED_PATH, f'{fname}')\n",
        "\n",
        "      cv2.imwrite(fname, pred_img_cv)\n",
        "\n",
        "ring_bell(f'\\nprediction done for {i} images')\n",
        "print(f'took {(time.time() - start_time):.2f}s')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f82idLOkpAc"
      },
      "source": [
        "# Predict FOR VAL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxnmvY65ksDT"
      },
      "source": [
        "VERBOSE = False #@param{type:'boolean'}\n",
        "\n",
        "PRED_PATH = os.path.join(HOME, 'predict-VAL', model_name)\n",
        "MAX_SAVED =  25#@param{type:'integer'}\n",
        "MAX_SAVED = min(MAX_SAVED, len(train_dataset))\n",
        "\n",
        "print(f'predicted images will be saved at {PRED_PATH}')\n",
        "PURGE = True #@param{type:'boolean'}\n",
        "if PURGE:\n",
        "  print('deleted prev. predictions')\n",
        "  ! rm -rf {PRED_PATH}\n",
        "os.makedirs(PRED_PATH, exist_ok=True)\n",
        "\n",
        "temps = [0.3, 1]\n",
        "print('starting prediction...\\n')\n",
        "\n",
        "model.eval()\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "  for i, data in tqdm(enumerate(val_dataset), desc='predict-V', total=MAX_SAVED):\n",
        "    if i >= MAX_SAVED:\n",
        "      break\n",
        "\n",
        "    Y, UV, p = data\n",
        "    orig_img = cv2.imread(p)\n",
        "\n",
        "    # save the orig image\n",
        "    orig_id = os.path.split(p)[-1].replace('.jpg', '-gt-thumb.jpg')\n",
        "    cv2.imwrite(os.path.join(PRED_PATH, orig_id), orig_img)\n",
        "\n",
        "    # normalize gt image prior to conversion from BGR\n",
        "    orig_img = np.float32(orig_img * 1./255)\n",
        "    # convert image to yuv in place, \n",
        "    orig_img = cv2.cvtColor(orig_img, cv2.COLOR_BGR2YCrCb)\n",
        "\n",
        "    # save the luminance of the original image\n",
        "    Y_orig = orig_img[..., 0]\n",
        "\n",
        "    # save the grayscale image as well\n",
        "    gray_id = os.path.split(p)[-1].replace('.jpg', '-gray.jpg')\n",
        "    cv2.imwrite(os.path.join(PRED_PATH, gray_id), Y_orig * 255)\n",
        "\n",
        "    Y = Y.float().to(DEVICE).unsqueeze(0)\n",
        "    Z = torch.softmax(model(Y), -1).cpu().numpy()\n",
        "\n",
        "    for t in temps:\n",
        "      UV = uv_px_from_z(Z, t).squeeze()\n",
        "      \n",
        "      pred_img = np.concatenate((np.expand_dims(Y_orig, -1), UV), axis=-1)\n",
        "\n",
        "      pred_img_cv = cv2.cvtColor(pred_img, cv2.COLOR_YCrCb2RGB)\n",
        "      pred_img_cv = np.clip(pred_img_cv, 0, 1) * 255\n",
        "\n",
        "      fname = os.path.split(p)[-1].replace('.jpg', f'-pred-{t}.jpg')\n",
        "      fname = os.path.join(PRED_PATH, f'{fname}')\n",
        "\n",
        "      cv2.imwrite(fname, pred_img_cv)\n",
        "\n",
        "ring_bell(f'\\nprediction done for {i} images')\n",
        "print(f'took {(time.time() - start_time):.2f}s')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dssyijCwpIZK"
      },
      "source": [
        "## Cleanup gpu memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HM2uvA2UwSEE"
      },
      "source": [
        "try:\n",
        "  del model\n",
        "  del optimizer\n",
        "  del criterion\n",
        "  \n",
        "except NameError:\n",
        "  pass\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "! nvidia-smi | grep MiB"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}